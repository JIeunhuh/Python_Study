{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(x_train words): 2494\n",
      "max(x_test words): 2315\n",
      "x_train[0]: [2, 2, 22, 16, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 100, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 17, 2, 38, 2, 2, 2, 2, 50, 16, 2, 2, 2, 19, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 16, 43, 2, 38, 76, 15, 2, 2, 2, 22, 17, 2, 17, 2, 16, 2, 18, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 16, 2, 66, 2, 33, 2, 2, 2, 16, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 16, 82, 2, 2, 2, 2, 2, 2, 15, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 15, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 18, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 18, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 16, 38, 2, 88, 2, 16, 2, 2, 16, 2, 2, 2, 32, 15, 16, 2, 19, 2, 32]\n",
      "y_train[0]: 1\n",
      "review of x_train[0]:\n",
      "? ? film was just ? ? ? ? story ? ? really ? ? ? they ? ? you could just ? ? there ? ? ? an ? ? ? ? ? ? ? ? ? ? ? from ? ? ? ? as ? so ? ? ? ? there was ? ? ? with ? film ? ? ? ? ? film were great ? was just ? so much that ? ? ? film as ? as ? was ? for ? ? would ? ? ? ? ? ? ? ? ? ? was ? really ? at ? ? ? was so ? ? you ? what they ? if you ? at ? film ? ? have been good ? ? ? was also ? ? ? ? ? ? that ? ? ? ? ? ? ? they were just ? ? are ? ? out ? ? ? ? ? ? because ? ? that ? them all ? up are ? ? ? ? for ? ? film but ? ? are ? ? ? be ? for what they have ? don't you ? ? ? story was so ? because ? was ? ? was ? ? ? all that was ? with ? all\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "#1\n",
    "##(x_train, y_train), (x_test, y_test) = imdb.load_data() # index_from=3\n",
    "\n",
    "#2\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(skip_top=15, num_words=101)\n",
    "\n",
    "##print(\"x_train.shape=\",x_train.shape) # (25000,)\n",
    "##print(\"y_train.shape=\",y_train.shape) # (25000,)\n",
    "##print(\"x_test.shape=\", x_test.shape)  # (25000,)\n",
    "##print(\"y_test.shape=\", y_test.shape)  # (25000,)\n",
    "\n",
    "#3\n",
    "##nlabel, count = np.unique(y_train, return_counts=True)\n",
    "##print(\"nlabel:\", nlabel)\n",
    "##print(\"count:\",  count)\n",
    "##print(\"# of Class:\",  len(nlabel) ) # 2\n",
    "\n",
    "print(\"max(x_train words):\", max(len(x) for x in x_train)) # 2494\n",
    "print(\"max(x_test words):\",  max(len(x) for x in x_test)) # 2315\n",
    "print(\"x_train[0]:\", x_train[0])\n",
    "print(\"y_train[0]:\", y_train[0])\n",
    "\n",
    "#4: decoding x_train[n], reverse from integers to words\n",
    "# ref: https://builtin.com/data-science/how-build-neural-network-keras\n",
    "n = 0\n",
    "index = imdb.get_word_index()\n",
    "reverse_index  = dict([(value, key) for (key, value) in index.items()]) \n",
    "review = \" \".join( [reverse_index.get(i-3, \"?\") for i in x_train[n]] )\n",
    "print(\"review of x_train[{}]:\\n{}\".format(n, review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.word_index: {'this': 1, 'is': 2, 'a': 3, 'film': 4, 'not': 5}\n",
      "sequences: [[1, 2, 3, 4], [1, 2, 5, 3, 4]]\n",
      "output_vector.shape= (2, 10)\n",
      "[[0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#1\n",
    "texts = ['This is a film','This is not a film']\n",
    "top_words= 10 # maximum integer index + 1\n",
    "\n",
    "#2\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(\"tokenizer.word_index:\",tokenizer.word_index)\n",
    "\n",
    "#3\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"sequences:\",sequences)\n",
    "\n",
    "#4: each vector length: top_words = 10\n",
    "output_vector = tokenizer.sequences_to_matrix(sequences) # mode='binary'\n",
    "print(\"output_vector.shape=\", output_vector.shape) # (2, 10)\n",
    "print(output_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.word_index: {'this': 1, 'is': 2, 'a': 3, 'film': 4, 'not': 5}\n",
      "sequences: [[1, 2, 3, 4], [1, 2, 5, 3, 4]]\n",
      "sequences.shape= (2, 6)\n",
      "sequences: [[0 0 1 2 3 4]\n",
      " [0 1 2 5 3 4]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 3)           30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "output_vector.shape: (2, 6, 3)\n",
      "output_vector: [[[-0.04269281 -0.03203346 -0.03257723]\n",
      "  [-0.04269281 -0.03203346 -0.03257723]\n",
      "  [ 0.01026332 -0.04542815 -0.02723212]\n",
      "  [-0.00529214  0.01892594 -0.03961267]\n",
      "  [ 0.04778251  0.04758165  0.01598446]\n",
      "  [ 0.00717454  0.03144721 -0.01134453]]\n",
      "\n",
      " [[-0.04269281 -0.03203346 -0.03257723]\n",
      "  [ 0.01026332 -0.04542815 -0.02723212]\n",
      "  [-0.00529214  0.01892594 -0.03961267]\n",
      "  [-0.0065892   0.02419173  0.00731269]\n",
      "  [ 0.04778251  0.04758165  0.01598446]\n",
      "  [ 0.00717454  0.03144721 -0.01134453]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#1\n",
    "texts = ['This is a film','This is not a film']\n",
    "top_words    = 10  # maximum integer index + 1\n",
    "max_words    = 6  # sequences.shape[1]\n",
    "vecor_length = 3  # dimension of the dense embedding\n",
    "\n",
    "#2\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(\"tokenizer.word_index:\",tokenizer.word_index)\n",
    "\n",
    "#3\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"sequences:\",sequences)\n",
    "\n",
    "#4\n",
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, max_words)\n",
    "print('sequences.shape=', sequences.shape)\n",
    "print(\"sequences:\", sequences)\n",
    "\n",
    "#5\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim =top_words, output_dim=vecor_length))\n",
    "##model.add(tf.keras.layers.Flatten()) # output_vector.shape = (2, 18)\n",
    "model.summary()\n",
    "\n",
    "#6\n",
    "output_vector = model.predict(sequences)\n",
    "print(\"output_vector.shape:\", output_vector.shape )\n",
    "print(\"output_vector:\", output_vector )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
